#!/usr/bin/python2.7

from datetime import date, timedelta
from datetime import datetime
import logging
import traceback
import database as db

from index import find_ref
import arxiv

logging.basicConfig(level=logging.DEBUG)

def oai_import(arxiv_id, tag=[]):
        arxiv.lookup_arxiv(arxiv_id)
        d = find_ref(ref)
        if d is None:
                new += 1
                d = {}
        d.update(ref)
        d['_id'] = 'arxiv:%s' % d['arxiv_id']
        if args.dryrun:
                print d
        else:
                for t in tag:
                        d.setdefault('tags', []).append({'name': t})
                db.refs.save(d)

def oai_import_set(args, wait=30, tag=[]):
        """ Imports records going back in time """
        day = args.start
        chunk_n = 1
        while True:
                _until = date.today() - timedelta(days=day)
                _from = date.today() - timedelta(days=day+args.chunksize)
                logging.info('Starting chunk %d: %s to %s' %
                             (chunk_n,
                              db.format_date(_from),
                              db.format_date(_until)))

                refs = arxiv.list_all_records(arxiv.arxiv_oai_url, wait=wait,
                                              _set=args.set,
                                              _from=_from, _until=_until)

                t0 = time.time()
                nrefs = 0
                new = 0
                for ref in refs:
                        d = None
                        try:
                                d = find_ref(ref)
                        except Exception as e:
                                logging.warn('Error processing %s: %s' % (ref, e))
                                logging.debug(traceback.format_exc())
                                continue

                        if d is None:
                                new += 1
                                d = {}
                        d.update(ref)
                        d['_id'] = 'arxiv:%s' % d['arxiv_id']
                        d['tags'] = [{'name': c} for c in d['arxiv_categories']]
                        for t in tag:
                                d['tags'].append({'name': t})

                        if args.dryrun:
                                print d
                        else:
                                try:
                                        db.refs.save(d)
                                except Exception as e:
                                        logging.warn('Error storing %s: %s' % (d, e))
                                        logging.debug(traceback.format_exc())
                                        continue
                        nrefs += 1

                dt = time.time()-t0
                logging.info('Chunk %d done: %d refs processed (%d new) in %f seconds (%f refs per second)' %
                              (chunk_n, nrefs, new, dt, 1.*nrefs/dt))
                day += args.chunksize
                
                logging.info('Sleeping 30 seconds')
                time.sleep(wait)
                chunk_n += 1

def list_sets(args):
        for spec,name in arxiv.list_sets(arxiv.arxiv_oai_url):
                print '%20s\t%s' % (spec, name)

if __name__ == '__main__':
        import sys
        import argparse
        import time

        parser = argparse.ArgumentParser(description='Import refs from arxiv')
        sps = parser.add_subparsers(help='Sub-command help')
        p = sps.add_parser('list-sets', help='List available sets')
        p.set_defaults(func=list_sets)

        p = sps.add_parser('import', help='Import single arXiv ID')
        p.add_argument('-n', '--dryrun',
                       help="Don't add to database, just print records to be imported")
        p.add_argument('arxiv_id', metavar='arXiv-ID',
                       help='arXiv ID of the reference to be imported')
        p.add_argument('-t', '--tag', metavar='TAG', action='append',
                       help='Add tag')
        p.set_defaults(func=oai_import)

        p = sps.add_parser('import-set', help='Import sets of references from OAI')
        p.add_argument('-C' ,'--chunksize', metavar='DAYS', type=int,
                       help='Size of each query')
        p.add_argument('-u', '--url', default=arxiv.arxiv_oai_url,
                       help='OAI query URL')
        p.add_argument('-s' ,'--start', metavar='DAYS', type=int,
                       help='Number of days ago to start downloading from')
        p.add_argument('-S', '--set', metavar='SET', type=str,
                       help='Set from which to import', default='physics:cond-mat')
        p.add_argument('-n', '--dryrun', action='store_true',
                       help="Don't add to database, just print records to be imported")
        p.add_argument('-t', '--tag', metavar='TAG', action='append',
                       help='Add tag')
        p.set_defaults(func=oai_import_set)

        args = parser.parse_args()
        args.func(args)

