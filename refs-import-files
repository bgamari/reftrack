#!/usr/bin/python2.7

import sys
import logging
import hashlib
import datetime
import database as db
import traceback

import index
import pdfscrape
import arxiv
import crossref

logging.basicConfig(level=logging.DEBUG)

total = 0
unidentified = []

def scrape_pdf(fname):
        global total, unidentified
        metad = pdfscrape.find_ids(fname)
        doi = metad.get('doi')
        arxiv_id = metad.get('arxiv_id')

        if doi:
                logging.debug('DOI of %s is %s' % (f, doi))
                m = crossref.lookup_doi(doi)
                if m:
                        metad['_id'] = 'doi:%s' % doi
                        metad.update(m)
                else:
                        logging.info('Failed to get crossref metadata for %s' % f)

        elif arxiv_id:
                logging.debug('arXiv ID of %s is %s' % (f, arxiv_id))
                m = arxiv.lookup_arxiv(arxiv_id)
                if m:
                        metad['_id'] = 'arxiv:%s' % arxiv_id
                        metad.update(m)
                else:
                        logging.info('Failed to get arXiv metadata for %s' % f)

        else:
                unidentified.append(fname)
                logging.info('Could not identify %s' % f)

        total += 1
        return metad

def process_file(f):
        logging.info('Processing file %s' % f)
        h = hashlib.md5()
        h.update(open(f).read())
        md5 = h.hexdigest()

        ref = None
        try:
                ref = scrape_pdf(f)
        except Exception as e:
                logging.warn('Error processing %s: %s' % (f, e))
                logging.debug(traceback.format_exc())
                return

        if 'title' not in ref or 'authors' not in ref:
                logging.warn('Insufficient metadata for %s' % f)
                return

        try:
                dbref = index.find_ref(ref)
        except Exception as e:
                logging.warn('Error processing %s: %s' % (f, e))
                logging.debug(traceback.format_exc())
                return

        refid = None
        if dbref is None:
                ref = db.refs.save(ref)
                refid = ref.id
        else:
                refid = dbref['_id']

        file = db.docs.get(md5)
        if file is None:
                file = {
                        '_id': md5,
                        'md5': md5,
                        'ref': refid,
                        'filename': f,
                        'date_imported': db.format_date(datetime.date.today()),
                }
                db.docs.save(file)
        elif file['ref'] != refid:
                logging.info('%s (md5=%s): existing file %s in database already already assigned to ref %s' %
                             (f, md5, file['filename'], ref['_id']))
        else:
                logging.info('%s (md5=%s): File already associated with ref %s' %
                             (f, md5, refid))

if __name__ == '__main__':
        import os
        import argparse

        parser = argparse.ArgumentParser(description='Import papers')
        parser.add_argument('paths', metavar='FILE_OR_DIR', nargs='+', type=str,
                            help='Paths to import')
        args = parser.parse_args()

        for p in args.paths:
                if os.path.isdir(p):
                        for (dirpath, dirnames, filenames) in os.walk(p):
                                for f in filenames: process_file(os.path.join(dirpath,f))
                elif os.path.isfile(p):
                        process_file(p)
        if total > 0:
                logging.info('Processed %d files, %d unidentified (identified %3.1f%%)' %
                             (total, len(unidentified), 100.*(total-len(unidentified))/total))

        with open('undentified.txt', 'w') as f:
                f.write('\n'.join(unidentified))

